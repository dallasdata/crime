#!/usr/bin/env python3
#
# The MIT License (MIT)
#
# Copyright (c) 2015 dallasdata
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
# Ideas:
#
#   - Record sanitized address information from PostGIS rather than the raw
#     contents of the XML
#
#   - Handle 1xx addresses where only the block number is recorded by emitting
#     a confidence interval (somehow)
#
#   - Use trigram or soundex capabilities in Postgres to match know-extant
#     streets and their types (from TIGER) with mashed up junk from XML dumps.
#     Even easier, create a list of kown-extant streets from TIGER concatenated
#     with their suffixes (or expanded list of all possible suffixes) and use
#     that to figure out suffixes matches. With the list of queries (from XMLs)
#     and known-goods (from TIGER) each sorted, this should be O(N) to get good
#     matches. Will also help with 14-character truncation woes.
#
#   - Some input files contain invalid XML entities (e.g.
#     Offense20080608095354.xml); fix these either by hand or programatically
#     massage them at runtime

'''
Transform the XML archives from prior to June 1 2014 into a CSV file.
'''

import argparse
import contextlib
import csv
import datetime
import functools
import logging
import os.path
from postgres import Postgres
import sys
import xml.sax

STREET_SUFFIXES = [
        'AV',
        'AVE',
        'BL',
        'BLVD',
        'CIR',
        'CT',
        'DR',
        'EXT',
        'FRW',
        'FRWY',
        'HWY',
        'LA',
        'LN',
        'PKW',
        'PKWY',
        'PL',
        'PWY',
        'RD',
        'RDG',
        'ST',
        'TR',
        'TRAIL',
        'TRL',
        'WAY',
        'WY',]


# Context manager for opening files (or stdin)
@contextlib.contextmanager
def open_path(fp, mode='r'):
    assert mode in ['r', 'w']

    if fp == '-':
        if mode == 'r':
            yield sys.stdin
        else:
            yield sys.stdout
    else:
        f = open(fp, mode)
        yield f
        f.close()

# DPDSAXContentHandler callback for writing a record to a CSV file
def xml_write_record_callback(cw, dcw, db, xr, loc):
    dr = {}

    try:
        d = datetime.datetime.strptime(xr['offensedateofoccurence1'], '%m/%d/%Y')
        dr['start_date_year'] = d.strftime('%Y')
        dr['start_date_month'] = d.strftime('%m')
        dr['start_date_day'] = d.strftime('%d')
    except:
        pass

    try:
        d = datetime.datetime.strptime(xr['offensetimeofoccurence1'], '%H:%M')
        dr['start_time_hour'] = d.strftime('%H')
        dr['start_time_min'] = d.strftime('%M')
    except:
        pass

    try:
        d = datetime.datetime.strptime(xr['offensedateofoccurence2'], '%m/%d/%Y')
        dr['end_date_year'] = d.strftime('%Y')
        dr['end_date_month'] = d.strftime('%m')
        dr['end_date_day'] = d.strftime('%d')
    except:
        pass

    try:
        d = datetime.datetime.strptime(xr['offensetimeofoccurence2'], '%H:%M')
        dr['end_time_hour'] = d.strftime('%H')
        dr['end_time_min'] = d.strftime('%M')
    except:
        pass

    try:
        d = datetime.datetime.strptime(xr['offensereporteddate'], '%m/%d/%Y')
        dr['report_date_year'] = d.strftime('%Y')
        dr['report_date_month'] = d.strftime('%m')
        dr['report_date_day'] = d.strftime('%d')
    except:
        pass

    try:
        dr['street_number'] = int(xr.get('offenseblock'))
    except:
        pass

    dr['street'] = xr.get('offensestreet')
    for ss in STREET_SUFFIXES:
        if dr['street'].endswith(ss):
            dr['street'] = dr['street'][:-1 * len(ss)] + ' ' + ss
            break
    else:
        logging.debug('No suffix for {}'.format(dr['street']))

    try:
        dr['apartment'] = int(xr.get('offenseapartment'))
    except:
        pass

    dr['city'] = xr.get('offensecity')
    dr['state'] = xr.get('offensestate')
    dr['zipcode'] = xr.get('offensezip')
    dr['offense_description'] = xr.get('offensedescription')

    # If we have an address, attempt to geocode it
    if 'street_number' in dr and 'street' in dr and db:
        r = db.one("select g.rating, ST_X(g.geomout), ST_Y(g.geomout) from geocode('{street_number} {street}, {city}, {state} {zip}', 1) as g;".format(
            street_number=dr['street_number'],
            street=dr['street'],
            city=dr['city'],
            state=dr['state'],
            zip=dr['zipcode']))

        # Skip results with confidence worse than 10. The range for this
        # value is 0-100?, with lower is more confident)
        if r is not None and r.rating <= 10:
            dr['longitude'] = r.st_x
            dr['latitude'] = r.st_y

    cw.writerow(dr)
    dcw.writerow(loc)


# SAX content handler for DPD crimes
class DPDSAXContentHandler(xml.sax.ContentHandler):
    def __init__(self, cb, path):
        self.callback = cb
        self.path = path
        self.in_record = False
        self.record_loc = None
        self.attr_name = None
        self.attr_content = ''
        self.record_attrs = {}
        self.locator = None

    def setDocumentLocator(self, loc):
        self.locator = loc

    def startElement(self, name, attrs):
        name = name.lower()

        if not self.in_record:
            self.in_record = name == 'record'
            if self.in_record:
                self.record_loc = {
                        'path': self.path,
                        'line': self.locator.getLineNumber(),
                        'col': self.locator.getColumnNumber()}
        else:
            assert name != 'record'
            self.attr_name = name

    def characters(self, content):
        self.attr_content += content

    def endElement(self, name):
        name = name.lower()

        if name == self.attr_name:
            self.record_attrs[self.attr_name] = self.attr_content.strip()
            self.attr_name = None
            self.attr_content = ''

        if name == 'record':
            assert self.in_record
            self.callback(self.record_attrs, self.record_loc)
            self.in_record = False
            self.record_loc = None


# SAX error handler
class DPDSAXErrorHandler(xml.sax.ErrorHandler):
    def warning(self, ex):
        logging.warning(ex)

    def error(self, ex):
        logging.error(ex)

    def fatalError(self, ex):
        raise ex


ap = argparse.ArgumentParser(
        description='''
Convert incident data from prior to June 1, 2014 to our normalized CSV format.
''')
ap.add_argument(
        '-o', metavar='<file>', dest='output_path', action='store', default='-',
        help='''
file to write CSV output to; defaults to stdout
''')
ap.add_argument(
        '-v', '--verbose', action='count', dest='verbosity', default=0,
        help='''
increase global logging verbosity; can be used multiple times
''')
ap.add_argument('--geocode', action='store_true', default=False,
        help='''
geocode locations found in source files
''')
ap.add_argument('--debug-file', action='store_true', default=False,
        help='''
write debug files next to output files
''')
ap.add_argument('input_paths', nargs='*',
        help='''
file(s) from which to read XML dumps; if empty stdin is used
''')
args = ap.parse_args()

logging.basicConfig(
        level=logging.ERROR - args.verbosity * 10,
        style='{',
        format='{asctime} {levelname} [{name}]: {message}')

if len(args.input_paths) == 0:
    args.input_paths += ['-']

db = None
if args.geocode:
    db = Postgres('postgres://geocoder:geocoder@localhost/dallas')

debug_path = '/dev/null'
if args.debug_file:
    if args.output_path == '-':
        logging.warning('unable to create debug file for stdout')
    else:
        debug_path = args.output_path + '.debug'

with open_path(args.output_path, 'w') as outf, \
        open_path(debug_path, 'w') as debugf:
    cw = csv.DictWriter(outf, [
          'start_date_year',
          'start_date_month',
          'start_date_day',
          'start_time_hour',
          'start_time_min',
          'end_date_year',
          'end_date_month',
          'end_date_day',
          'end_time_hour',
          'end_time_min',
          'report_date_year',
          'report_date_month',
          'report_date_day',
          'report_time_hour',
          'report_time_min',
          'street_number',
          'street',
          'apartment',
          'city',
          'state',
          'zipcode',
          'longitude',
          'latitude',
          'offense_description',
      ])
    cw.writeheader()

    dcw = csv.DictWriter(debugf, ['path', 'line', 'col'])
    dcw.writeheader()

    for fp in args.input_paths:
        logging.warning('processing {}'.format(fp))

        xp = xml.sax.make_parser()
        xp.setErrorHandler(DPDSAXErrorHandler())
        xp.setContentHandler(
                DPDSAXContentHandler(
                    functools.partial(xml_write_record_callback, cw, dcw, db),
                    fp))

        with open_path(fp, 'r') as inf:
            xp.parse(inf)
